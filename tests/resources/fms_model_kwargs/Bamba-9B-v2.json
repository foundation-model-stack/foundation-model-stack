{
    "activation_fn": "silu",
    "architecture": "bamba",
    "chunk_size": 256,
    "conv_kernel": 4,
    "emb_dim": 4096,
    "head_dim": 64,
    "hidden_grow_factor": 3.5,
    "kvheads": 8,
    "mamba_expand": 2,
    "mamba_n_heads": 128,
    "model_path": null,
    "n_groups": 1,
    "nheads": 32,
    "nlayers": 32,
    "norm_eps": 1e-05,
    "p_dropout": 0.0,
    "src_vocab_size": 128256,
    "state_size": 128,
    "tie_heads": false,
    "use_bias": false,
    "use_conv_bias": true,
    "variant": "9_8b"
}