{
    "activation_fn": "silu",
    "architecture": "granite",
    "attention_multiplier": 0.015625,
    "attn_bias": false,
    "emb_dim": 2048,
    "embedding_multiplier": 12.0,
    "head_dim": 64,
    "hidden_grow_factor": 4.0,
    "kvheads": 8,
    "logits_scaling": 8.0,
    "max_expected_seq_len": 131072,
    "mlp_bias": false,
    "model_path": null,
    "multiple_of": 1,
    "nheads": 32,
    "nlayers": 40,
    "norm_eps": 1e-05,
    "residual_multiplier": 0.22,
    "rope_theta": 10000000.0,
    "src_vocab_size": 49159,
    "tie_heads": true,
    "variant": "8b"
}