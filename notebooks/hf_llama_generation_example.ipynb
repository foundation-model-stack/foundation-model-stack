{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632a649d",
   "metadata": {},
   "source": [
    "# Simple HuggingFace inference with Huggingface Adapted FMS LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36289ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from fms.models import llama\n",
    "import torch\n",
    "from fms.models.hf.llama.modeling_llama_hf import LLaMAHFForCausalLM\n",
    "from transformers import LlamaForCausalLM, pipeline, AutoTokenizer\n",
    "from fms.models.hf.utils import register_fms_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b5344",
   "metadata": {},
   "source": [
    "## load Huggingface LLaMA model\n",
    "\n",
    "Simply load the Huggingface LLaMA model from a path containing the Huggingface LLaMA checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7163f0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb152cfcda2043a8ae02624383f46647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"/path/to/hf_model\"\n",
    "hf_model = LlamaForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef52faba",
   "metadata": {},
   "source": [
    "## convert Huggingface LLaMA model to FMS LLaMA model\n",
    "\n",
    "fms provides a simple function which will convert a pre-trained Huggingface LLaMA model to a pre-trained FMS LLaMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c016800",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llama.convert_hf_llama(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132e7e4",
   "metadata": {},
   "source": [
    "## Convert FMS LLaMA model to its Huggingface adapted FMS LLaMA model\n",
    "\n",
    "all fms models can be wrapped to act like a Huggingface model with a single line, but with performance benefits of the underlying fms model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58cc13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register LLaMAHFForCausalLM with AutoModel\n",
    "register_fms_models()\n",
    "# convert FMS LLaMA to HF-Adapted FMS LLaMA\n",
    "model = LLaMAHFForCausalLM.from_fms_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c816c",
   "metadata": {},
   "source": [
    "## Simple inference with Huggingface pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01bf9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "149e9ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I believe the meaning of life is to find happiness and fulfillment. Here are some of the things that bring me joy and fulfillment:\\n\\n'}]\n",
      "36.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "pipe = pipeline(task=\"text-generation\", model=model, max_new_tokens=25, tokenizer=tokenizer)\n",
    "prompt = \"\"\"I believe the meaning of life is\"\"\"\n",
    "result = pipe(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08e175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cb14521",
   "metadata": {},
   "source": [
    "## Compilation\n",
    "\n",
    "all fms models support torch compile for faster inference, therefore Huggingface Adapted FMS models also support this feature. \n",
    "\n",
    "*Note: `generate` calls the underlying decoder and not the model itself, which requires compiling the underlying decoder.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a24655fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder = torch.compile(model.decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfd3ce",
   "metadata": {},
   "source": [
    "Because compile is lazy, we first just do a single generation pipeline to compile the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = pipeline(task=\"text-generation\", model=model, max_new_tokens=25, tokenizer=tokenizer)\n",
    "# prompt = \"\"\"I believe the meaning of life is\"\"\"\n",
    "# result = pipe(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b22dc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I believe the meaning of life is to find happiness and fulfillment. Here are some of the things that bring me joy and fulfillment:\\n\\n'}]\n",
      "26.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "pipe = pipeline(task=\"text-generation\", model=model, max_new_tokens=25, tokenizer=tokenizer)\n",
    "prompt = \"\"\"I believe the meaning of life is\"\"\"\n",
    "result = pipe(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd80ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
