{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632a649d",
   "metadata": {},
   "source": [
    "# Simple HuggingFace inference with Huggingface Adapted FMS LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1500f8",
   "metadata": {},
   "source": [
    "*Note: This notebook is using Torch 2.1.0 and Transformers 4.35.0.dev0*\n",
    "\n",
    "If you would like to run a similar pipeline using a script, please view the following file: `scripts/hf_compile_example.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36289ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from fms.models import llama\n",
    "import torch\n",
    "from fms.models.hf.llama.modeling_llama_hf import HFAdaptedLLaMAForCausalLM\n",
    "from transformers import LlamaForCausalLM, pipeline, AutoTokenizer\n",
    "from fms.models.hf.utils import register_fms_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b5344",
   "metadata": {},
   "source": [
    "## load Huggingface LLaMA model\n",
    "\n",
    "Simply load the Huggingface LLaMA model from a path containing the Huggingface LLaMA checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7163f0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6f9d18b9b34eaea18963b68f814eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"/Users/joshuarosenkranz/Documents/desktop cleanup/10_14_23/llama_models/7B-F\"\n",
    "hf_model = LlamaForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef52faba",
   "metadata": {},
   "source": [
    "## convert Huggingface LLaMA model to FMS LLaMA model\n",
    "\n",
    "fms provides a simple function which will convert a pre-trained Huggingface LLaMA model to a pre-trained FMS LLaMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c016800",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llama.convert_hf_llama(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132e7e4",
   "metadata": {},
   "source": [
    "## Convert FMS LLaMA model to its Huggingface adapted FMS LLaMA model\n",
    "\n",
    "all fms models can be wrapped to act like a Huggingface model with a single line, but with performance benefits of the underlying fms model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58cc13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register LLaMAHFForCausalLM with AutoModel\n",
    "register_fms_models()\n",
    "# convert FMS LLaMA to HF-Adapted FMS LLaMA\n",
    "model = HFAdaptedLLaMAForCausalLM.from_fms_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c816c",
   "metadata": {},
   "source": [
    "## Simple inference with Huggingface pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01bf9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "149e9ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I believe the meaning of life is to find happiness and fulfillment. Here are some of the things that bring me joy and fulfillment:\\n\\n'}]\n",
      "22.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "pipe = pipeline(task=\"text-generation\", model=model, max_new_tokens=25, tokenizer=tokenizer)\n",
    "prompt = \"\"\"I believe the meaning of life is\"\"\"\n",
    "result = pipe(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08e175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cb14521",
   "metadata": {},
   "source": [
    "## Compilation\n",
    "\n",
    "all fms models support torch compile for faster inference, therefore Huggingface Adapted FMS models also support this feature. \n",
    "\n",
    "*Note: `generate` calls the underlying decoder and not the model itself, which requires compiling the underlying decoder.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fae68",
   "metadata": {},
   "source": [
    "*including a compile counter to demonstrate that the model is being compiled*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "724ca5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._dynamo.testing import CompileCounterWithBackend\n",
    "torch._dynamo.reset()\n",
    "cnt = CompileCounterWithBackend(\"inductor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ae42d",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a24655fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder = torch.compile(model=model.decoder, backend=cnt)\n",
    "assert cnt.frame_count == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfd3ce",
   "metadata": {},
   "source": [
    "Because compile is lazy, we first just do a single generation pipeline to compile the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, max_new_tokens=25, tokenizer=tokenizer)\n",
    "prompt = \"\"\"I believe the meaning of life is\"\"\"\n",
    "result = pipe(prompt)\n",
    "assert cnt.frame_count > 0, \"model did not get compiled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55206d51",
   "metadata": {},
   "source": [
    "At this point, the graph should be compiled and we can get proper performance numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b22dc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I believe the meaning of life is to find happiness and fulfillment. Here are some of the things that bring me joy and fulfillment:\\n\\n'}]\n",
      "21.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "pipe = pipeline(task=\"text-generation\", model=model, max_new_tokens=25, tokenizer=tokenizer)\n",
    "prompt = \"\"\"I believe the meaning of life is\"\"\"\n",
    "result = pipe(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd80ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
