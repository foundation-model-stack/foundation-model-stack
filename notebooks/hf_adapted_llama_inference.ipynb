{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632a649d",
   "metadata": {},
   "source": [
    "# Simple HuggingFace inference with Huggingface Adapted FMS LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1500f8",
   "metadata": {},
   "source": [
    "*Note: This notebook is using Torch 2.1.0 and Transformers 4.35.0.dev0*\n",
    "\n",
    "If you would like to run a similar pipeline using a script, please view the following file: `scripts/hf_compile_example.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36289ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from fms.models.hf.llama import get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b5344",
   "metadata": {},
   "source": [
    "## load Huggingface Adapted FMS LLaMA model\n",
    "\n",
    "Simply get the Huggingface Llama model and convert it to an HF adapted Llama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b02ec29-289e-425a-960e-a66d6521730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/path/to/hf_llama_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63af00ae-b041-4e2f-b882-c99389d967cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8e901cbc784aa0b4412f195a3b5593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_model(model_path).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c816c",
   "metadata": {},
   "source": [
    "## Simple inference with Huggingface pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01bf9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "149e9ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I believe the meaning of life is to find happiness and fulfillment. Here are some of the things that bring me joy and fulfillment:\\n\\n'}]\n",
      "1.36 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "pipe = pipeline(task=\"text-generation\", model=model, max_new_tokens=25, tokenizer=tokenizer, device=\"cuda\")\n",
    "prompt = \"\"\"I believe the meaning of life is\"\"\"\n",
    "result = pipe(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08e175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cb14521",
   "metadata": {},
   "source": [
    "## Compilation\n",
    "\n",
    "all fms models support torch compile for faster inference, therefore Huggingface Adapted FMS models also support this feature. \n",
    "\n",
    "*Note: `generate` calls the underlying decoder and not the model itself, which requires compiling the underlying decoder.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24655fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder = torch.compile(model.decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfd3ce",
   "metadata": {},
   "source": [
    "Because compile is lazy, we first just do a single generation pipeline to compile the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, max_new_tokens=25, tokenizer=tokenizer, device=\"cuda\")\n",
    "prompt = \"\"\"I believe the meaning of life is\"\"\"\n",
    "result = pipe(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55206d51",
   "metadata": {},
   "source": [
    "At this point, the graph should be compiled and we can get proper performance numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b22dc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I believe the meaning of life is to find happiness and fulfillment. Here are some of the things that bring me joy and fulfillment:\\n\\n'}]\n",
      "587 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "pipe = pipeline(task=\"text-generation\", model=model, max_new_tokens=25, tokenizer=tokenizer, device=\"cuda\")\n",
    "prompt = \"\"\"I believe the meaning of life is\"\"\"\n",
    "result = pipe(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd80ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
