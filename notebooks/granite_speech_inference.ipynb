{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c72e86765043d38",
   "metadata": {},
   "source": [
    "# GraniteSpeech Inference with FMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd43406407e4a99",
   "metadata": {},
   "source": [
    "This notebook demonstrates speech-to-text inference using GraniteSpeech model in FMS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2bbdcd96f74d7",
   "metadata": {},
   "source": [
    "## Environment Setup (External Environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e6c7ae6cad337",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/foundation-model-stack/foundation-model-stack.git\n",
    "%cd foundation-model-stack\n",
    "!git checkout main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8707f7f12ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e .\n",
    "!pip install datasets soundfile torchaudio huggingface_hub peft torchcodec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e03cb3819f26ab",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5930b9ad804934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from fms.models import get_model\n",
    "from fms.models.granite_speech import GraniteSpeechFeatureExtractor, GraniteSpeechProcessor\n",
    "from fms.utils.generation import generate\n",
    "from fms.utils.tokenizers import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0606c5e56bef6",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8fc37508464dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIGS = {\n",
    "    \"3.3-8b\": {\n",
    "        \"model_id\": \"ibm-granite/granite-speech-3.3-8b\",\n",
    "        \"variant\": \"3.3-8b\",\n",
    "        \"ignore_patterns\": None,\n",
    "    },\n",
    "    \"3.3-2b\": {\n",
    "        \"model_id\": \"ibm-granite/granite-speech-3.3-2b\",\n",
    "        \"variant\": \"3.3-2b\",\n",
    "\n",
    "        # TODO: Remove once IBM deletes orphaned 3-shard files from HF repo\n",
    "        \"ignore_patterns\": [\"*-of-00003.safetensors\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12b97356a3e3e5",
   "metadata": {},
   "source": [
    "## Prompt Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcdoqm4ox5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default prompts for GraniteSpeech\n",
    "SYSTEM_PROMPT = \"\"\"Knowledge Cutoff Date: April 2024.\n",
    "Today's Date: April 9, 2025.\n",
    "You are Granite, developed by IBM. You are a helpful AI assistant\"\"\"\n",
    "\n",
    "USER_PROMPT = \"<|audio|>can you transcribe the speech into a written format?\"\n",
    "\n",
    "\n",
    "def build_chat_prompt(\n",
    "    tokenizer: Any,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    user_prompt: str = USER_PROMPT,\n",
    ") -> str:\n",
    "    \"\"\"Build a chat-formatted prompt using the tokenizer's chat template.\"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmbc0593xy",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a4038e4ea02d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(\n",
    "    model_id: str,\n",
    "    variant: str,\n",
    "    device: str = \"cuda\",\n",
    "    dtype: torch.dtype = torch.bfloat16,\n",
    "    ignore_patterns: Optional[list] = None,\n",
    ") -> Tuple[torch.nn.Module, Any]:\n",
    "    model_path = snapshot_download(model_id, ignore_patterns=ignore_patterns)\n",
    "    model = get_model(\n",
    "        \"granite_speech\",\n",
    "        variant,\n",
    "        model_path=model_path,\n",
    "        source=\"hf\",\n",
    "        device_type=device,\n",
    "        data_type=dtype,\n",
    "    )\n",
    "    model.eval()\n",
    "    tokenizer_wrapper = get_tokenizer(model_id)\n",
    "    # Extract underlying HF tokenizer if wrapped\n",
    "    hf_tokenizer = getattr(tokenizer_wrapper, 'tokenizer', tokenizer_wrapper)\n",
    "    return model, hf_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g1sdnxf7m3k",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(\n",
    "    dataset_name: str = \"hf-internal-testing/librispeech_asr_dummy\",\n",
    "    split: str = \"validation\",\n",
    "    sample_index: int = 0,\n",
    ") -> Tuple[torch.Tensor, str, float]:\n",
    "    \"\"\"Load audio sample from LibriSpeech dataset.\"\"\"\n",
    "    dataset = load_dataset(dataset_name, \"clean\", split=split)\n",
    "    sample = dataset[sample_index]\n",
    "    audio = torch.tensor(sample[\"audio\"][\"array\"], dtype=torch.float32)\n",
    "    ground_truth = sample[\"text\"]\n",
    "    duration = len(audio) / sample[\"audio\"][\"sampling_rate\"]\n",
    "    return audio, ground_truth, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9a57eb4e75a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transcript(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: Any,\n",
    "    inputs: Dict[str, torch.Tensor],\n",
    "    max_new_tokens: int = 200,\n",
    ") -> str:\n",
    "    \"\"\"Generate transcription from audio inputs.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output_ids = generate(\n",
    "            model,\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            extra_kwargs={\n",
    "                \"input_features\": inputs[\"input_features\"],\n",
    "                \"input_features_mask\": inputs.get(\"input_features_mask\"),\n",
    "                \"attention_mask\": inputs.get(\"attention_mask\"),\n",
    "            },\n",
    "        )\n",
    "    # Strip input tokens to get only the generated response\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = output_ids[:, input_length:]\n",
    "    return tokenizer.batch_decode(new_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795015018f3ccfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inputs(\n",
    "    audio: torch.Tensor,\n",
    "    tokenizer: Any,\n",
    "    prompt: str,\n",
    "    device: str,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    processor = GraniteSpeechProcessor(GraniteSpeechFeatureExtractor(), tokenizer)\n",
    "    inputs = processor(text=[prompt], audio=audio, return_tensors=\"pt\")\n",
    "    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ob8jzcply58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(\n",
    "    model_config: str = \"3.3-8b\",\n",
    "    device: str = \"cuda\",\n",
    "    dtype: torch.dtype = torch.bfloat16,\n",
    "    sample_index: int = 0,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    user_prompt: str = USER_PROMPT,\n",
    "    max_new_tokens: int = 200,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Run speech-to-text inference with proper chat template formatting.\"\"\"\n",
    "    config = MODEL_CONFIGS[model_config]\n",
    "    \n",
    "    print(f\"Loading model: {config['model_id']}\")\n",
    "    model, tokenizer = get_model_and_tokenizer(\n",
    "        model_id=config[\"model_id\"],\n",
    "        variant=config[\"variant\"],\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "        ignore_patterns=config[\"ignore_patterns\"],\n",
    "    )\n",
    "    \n",
    "    print(\"Loading audio...\")\n",
    "    audio, ground_truth, duration = get_audio(sample_index=sample_index)\n",
    "    \n",
    "    # Build chat-formatted prompt\n",
    "    prompt = build_chat_prompt(tokenizer, system_prompt, user_prompt)\n",
    "    \n",
    "    print(\"Processing inputs...\")\n",
    "    inputs = process_inputs(audio, tokenizer, prompt, device)\n",
    "    \n",
    "    print(\"Generating transcription...\")\n",
    "    transcription = generate_transcript(model, tokenizer, inputs, max_new_tokens)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Ground Truth:  {ground_truth}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Transcription: {transcription.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\"ground_truth\": ground_truth, \"transcription\": transcription}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba8150607a5d2dc",
   "metadata": {},
   "source": [
    "## 8B Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243eb3bd08b1164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on multiple samples\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Test with sample indices 2, 5, 6 (matching reference script)\n",
    "results_8b = []\n",
    "for idx in [2, 5, 6]:\n",
    "    print(f\"\\n{'#'*30} SAMPLE {idx} {'#'*30}\")\n",
    "    result = run_inference(\n",
    "        model_config=\"3.3-8b\",\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "        sample_index=idx,\n",
    "    )\n",
    "    results_8b.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee7cd72d1c07",
   "metadata": {},
   "source": [
    "## 2B Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56828c368ff2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on multiple samples with 2B model\n",
    "results_2b = []\n",
    "for idx in [2, 5, 6]:\n",
    "    print(f\"\\n{'#'*30} SAMPLE {idx} {'#'*30}\")\n",
    "    result = run_inference(\n",
    "        model_config=\"3.3-2b\",\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "        sample_index=idx,\n",
    "    )\n",
    "    results_2b.append(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
